{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpd7GYm0iSNi",
        "outputId": "1699e8b8-9226-4ef5-a288-a32b3435ce9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-h6en2sqk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-h6en2sqk\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.9.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.6.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.4.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n",
            "--2024-12-20 06:12:26--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241220T061226Z&X-Amz-Expires=300&X-Amz-Signature=d18a777cc1f955c70ef7722b29389d30856735f8b39e422868e0e52478b9225d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-20 06:12:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241220T061226Z&X-Amz-Expires=300&X-Amz-Signature=d18a777cc1f955c70ef7722b29389d30856735f8b39e422868e0e52478b9225d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600444501 (573M) [application/octet-stream]\n",
            "Saving to: ‘s2v_reddit_2015_md.tar.gz.4’\n",
            "\n",
            "s2v_reddit_2015_md. 100%[===================>] 572.63M   108MB/s    in 7.9s    \n",
            "\n",
            "2024-12-20 06:12:35 (72.5 MB/s) - ‘s2v_reddit_2015_md.tar.gz.4’ saved [600444501/600444501]\n",
            "\n",
            "./._s2v_old\n",
            "./s2v_old/\n",
            "./s2v_old/._freqs.json\n",
            "./s2v_old/freqs.json\n",
            "./s2v_old/._vectors\n",
            "./s2v_old/vectors\n",
            "./s2v_old/._cfg\n",
            "./s2v_old/cfg\n",
            "./s2v_old/._strings.json\n",
            "./s2v_old/strings.json\n",
            "./s2v_old/._key2row\n",
            "./s2v_old/key2row\n",
            "time: 19.4 s (started: 2024-12-20 06:12:23 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet flashtext==2.7\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install --quiet transformers==4.8.1\n",
        "!pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2\n",
        "!pip install --quiet gradio==3.0.20\n",
        "!pip install --quiet strsim==0.0.3\n",
        "!pip install --quiet sense2vec==2.0.0\n",
        "!pip install --quiet ipython-autotime\n",
        "!pip install --upgrade huggingface_hub\n",
        "%load_ext autotime\n",
        "!pip install --quiet sentence-transformers==2.2.2\n",
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8WQPmEvx7Bvj",
        "outputId": "8b2608af-3922-458d-9176-e014c2e6f26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Collecting argparse (from anvil-uplink)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.17.0)\n",
            "Requirement already satisfied: ws4py-sslupdate in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.5.1b0)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "25479422828548bba9c042d6b7383b1d",
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.32 s (started: 2024-12-20 06:12:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "32c5_H7FiUIF",
        "outputId": "70c91405-1d3a-446a-bcb8-a0130dc45b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"summary_questions_distractors_generator\" as SERVER\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b95a28a5e353>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflashtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeywordProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentence-transformers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 14.2 s (started: 2024-12-20 06:12:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import anvil.server\n",
        "anvil.server.connect(\"server_EATVENTPOGH7AH25767NPNAD-YAJNNTPIKXVPEX4O\")\n",
        "import os\n",
        "import pke\n",
        "import torch\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from textwrap3 import wrap\n",
        "from sense2vec import Sense2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from collections import OrderedDict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from flashtext import KeywordProcessor\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7rTTNATTihWR"
      },
      "outputs": [],
      "source": [
        "import pke\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "@anvil.server.callable\n",
        "def ans(input_text):\n",
        "  class TextSummarizer:\n",
        "      def __init__(self, model_name='t5-base'):\n",
        "          \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "          self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.get_device())\n",
        "          self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "      def get_device(self):\n",
        "          \"\"\"Return the device GPU/CPU to use.\"\"\"\n",
        "          return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      def summarize_text(self, text, max_length=500, min_length=100):\n",
        "          text = text.strip().replace(\"\\n\", \" \")\n",
        "          text = \"summarize: \" + text\n",
        "          encoding = self.tokenizer.encode_plus(text, max_length=512, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(self.get_device())\n",
        "          outs = self.model.generate(input_ids=encoding[\"input_ids\"],\n",
        "                                    attention_mask=encoding[\"attention_mask\"],\n",
        "                                    early_stopping=True,\n",
        "                                    num_beams=5,\n",
        "                                    num_return_sequences=1,\n",
        "                                    no_repeat_ngram_size=3,\n",
        "                                    min_length=min_length,\n",
        "                                    max_length=max_length)\n",
        "\n",
        "          summary = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "          return self.post_process_text(summary)\n",
        "\n",
        "      @staticmethod\n",
        "      def post_process_text(content):\n",
        "          final = \"\"\n",
        "          for sent in sent_tokenize(content):\n",
        "              final += \" \" + sent.capitalize()\n",
        "          return final.strip()\n",
        "  text_summarizer = TextSummarizer()\n",
        "  sample_text = input_text\n",
        "  # Input from anvil\n",
        "  summarized_text = text_summarizer.summarize_text(sample_text)\n",
        "  print(\"Summarized Text:\", summarized_text)\n",
        "\n",
        "\n",
        "  class KeywordExtractor:\n",
        "      def __init__(self):\n",
        "          self.keyword_processor = KeywordProcessor()\n",
        "\n",
        "      @staticmethod\n",
        "      def extract_nouns_multipartite(content):\n",
        "          \"\"\"\n",
        "          Extracts nouns and proper nouns from the given content using MultipartiteRank algorithm.\n",
        "          \"\"\"\n",
        "          out = []\n",
        "          try:\n",
        "              extractor = pke.unsupervised.MultipartiteRank()\n",
        "              extractor.load_document(input=content, language='en')\n",
        "              pos = {'PROPN', 'NOUN'}\n",
        "              stoplist = list(string.punctuation) + stopwords.words('english')\n",
        "              extractor.candidate_selection(pos=pos)\n",
        "              extractor.candidate_weighting(alpha=1.5, threshold=0.80, method='average')\n",
        "              keyphrases = extractor.get_n_best(n=25)\n",
        "\n",
        "              for val in keyphrases:\n",
        "                  out.append(val[0])\n",
        "          except Exception:\n",
        "              pass\n",
        "\n",
        "          return out\n",
        "\n",
        "      def get_keywords(self, original_text, summary_text):\n",
        "          \"\"\"\n",
        "          Identifies important keywords from the original text and checks their presence in the summary.\n",
        "\n",
        "          Parameters:\n",
        "          original_text (str): The original text to extract keywords from.\n",
        "          summary_text (str): The summarized text to check against.\n",
        "\n",
        "          Returns:\n",
        "          list: A list of important keywords found in both original and summarized text.\n",
        "          \"\"\"\n",
        "          keywords = self.extract_nouns_multipartite(original_text)\n",
        "          print('Keywords in unsummarized : ', keywords)\n",
        "          for keyword in keywords:\n",
        "              self.keyword_processor.add_keyword(keyword)\n",
        "\n",
        "          keywords_found = list(set(self.keyword_processor.extract_keywords(summary_text)))\n",
        "          print('Keywords in summarized : ', keywords_found)\n",
        "\n",
        "          important_keywords = [keyword for keyword in keywords if keyword in keywords_found]\n",
        "\n",
        "          return important_keywords[:4]\n",
        "\n",
        "  keyword_extractor = KeywordExtractor()\n",
        "  keywords = keyword_extractor.get_keywords(sample_text, summarized_text)\n",
        "  print(\"Extracted Keywords:\", keywords)\n",
        "  class QuestionGenerator:\n",
        "      def __init__(self, model_name='ramsrigouthamg/t5_squad_v1'):\n",
        "          \"\"\"\n",
        "          Initializes the QuestionGenerator with a pre-trained model and tokenizer.\n",
        "          \"\"\"\n",
        "          self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.get_device())\n",
        "          self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "      def get_device(self):\n",
        "          return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      def generate_question(self, context, answer):\n",
        "          \"\"\"\n",
        "          Generates questions and answers based on the given context and model.\n",
        "\n",
        "          Parameters:\n",
        "          context (str): The context for the question generation.\n",
        "          answer (str): The answer to generate a question about.\n",
        "\n",
        "          Returns:\n",
        "          str: The generated question.\n",
        "          \"\"\"\n",
        "          text = f\"context: {context} answer: {answer}\"\n",
        "          encoding = self.tokenizer.encode_plus(text, max_length=384, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(self.get_device())\n",
        "\n",
        "          outs = self.model.generate(input_ids=encoding[\"input_ids\"],\n",
        "                                    attention_mask=encoding[\"attention_mask\"],\n",
        "                                    early_stopping=True,\n",
        "                                    num_return_sequences=2,\n",
        "                                    no_repeat_ngram_size=3,\n",
        "                                    num_beams=7,\n",
        "                                    max_length=75)\n",
        "\n",
        "          question = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "          return question.replace(\"question:\", \"\").strip()\n",
        "  class QuestionGenerator:\n",
        "      def __init__(self, model_name='ramsrigouthamg/t5_squad_v1'):\n",
        "          \"\"\"\n",
        "          Initializes the QuestionGenerator with a pre-trained model and tokenizer.\n",
        "          \"\"\"\n",
        "          self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.get_device())\n",
        "          self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "      def get_device(self):\n",
        "          return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      def generate_question(self, context, answer):\n",
        "          \"\"\"\n",
        "          Generates questions and answers based on the given context and model.\n",
        "\n",
        "          Parameters:\n",
        "          context (str): The context for the question generation.\n",
        "          answer (str): The answer to generate a question about.\n",
        "\n",
        "          Returns:\n",
        "          str: The generated question.\n",
        "          \"\"\"\n",
        "          text = f\"context: {context} answer: {answer}\"\n",
        "          encoding = self.tokenizer.encode_plus(text, max_length=384, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(self.get_device())\n",
        "\n",
        "          outs = self.model.generate(input_ids=encoding[\"input_ids\"],\n",
        "                                    attention_mask=encoding[\"attention_mask\"],\n",
        "                                    early_stopping=True,\n",
        "                                    num_return_sequences=2,\n",
        "                                    no_repeat_ngram_size=3,\n",
        "                                    num_beams=7,\n",
        "                                    max_length=75)\n",
        "\n",
        "          question = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "          return question.replace(\"question:\", \"\").strip()\n",
        "\n",
        "  question_generator = QuestionGenerator()\n",
        "  questions = [question_generator.generate_question(summarized_text, keyword) for keyword in keywords]\n",
        "  for question in questions:\n",
        "    print(\"Generated Question:\", question)\n",
        "\n",
        "  class DistractorGenerator:\n",
        "      def __init__(self, sense2vec_model, sentence_transformer_model):\n",
        "          \"\"\"\n",
        "          Initializes the DistractorGenerator with Sense2Vec and Sentence Transformer models.\n",
        "          \"\"\"\n",
        "          self.s2v = sense2vec_model\n",
        "          self.sentence_model = sentence_transformer_model\n",
        "          self.normalized_levenshtein = NormalizedLevenshtein()\n",
        "\n",
        "      def sense2vec_get_words(self, word, topn, question):\n",
        "          try:\n",
        "              sense = self.s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK OF ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
        "              most_similar = self.s2v.most_similar(sense, n=topn)\n",
        "              return self.filter_same_sense_words(sense, most_similar)\n",
        "          except Exception:\n",
        "              return []\n",
        "\n",
        "      @staticmethod\n",
        "      def filter_same_sense_words(original, wordlist):\n",
        "          base_sense = original.split('|')[1]\n",
        "          return [eachword[0].split('|')[0].replace(\"_\", \" \").title().strip() for eachword in wordlist if eachword[0].split('|')[1] == base_sense]\n",
        "\n",
        "      def get_distractors(self, word, sentence, top_n=40, lambda_param=0.2):\n",
        "          distractors = self.sense2vec_get_words(word, top_n, sentence)\n",
        "          if not distractors:\n",
        "              return []\n",
        "\n",
        "          distractors = [word.capitalize()] + distractors\n",
        "\n",
        "          sentence_embedding = self.sentence_model.encode([sentence + \" \" + word.capitalize()])\n",
        "          distractor_embeddings = self.sentence_model.encode(distractors)\n",
        "\n",
        "          filtered_keywords = self.mmr(sentence_embedding, distractor_embeddings, distractors, min(len(distractors), 5), lambda_param)\n",
        "\n",
        "          return [wrd for wrd in filtered_keywords if wrd.lower() != word.lower()][1:]\n",
        "\n",
        "      @staticmethod\n",
        "      def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
        "          \"\"\"\n",
        "          Applies the MMR (Maximal Marginal Relevance) algorithm to select top distractor words.\n",
        "\n",
        "          Parameters:\n",
        "          doc_embedding (np.array): The embedding of the sentence context.\n",
        "          word_embeddings (np.array): The embeddings of the distractor words.\n",
        "          words (list): The list of distractor words.\n",
        "          top_n (int): The number of top words to return.\n",
        "          lambda_param (float): Parameter for balancing relevance and diversity.\n",
        "\n",
        "          Returns:\n",
        "          list: A list of top distractor words based on MMR.\n",
        "          \"\"\"\n",
        "          word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "          word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "          keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "          candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "          for _ in range(top_n - 1):\n",
        "              candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "              target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "              mmr = (lambda_param * candidate_similarities) - ((1 - lambda_param) * target_similarities.reshape(-1, 1))\n",
        "              mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "              keywords_idx.append(mmr_idx)\n",
        "              candidates_idx.remove(mmr_idx)\n",
        "\n",
        "          return [words[idx] for idx in keywords_idx]\n",
        "\n",
        "  s2v_model = Sense2Vec().from_disk('s2v_old')\n",
        "  sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
        "  distractor_generator = DistractorGenerator(s2v_model, sentence_model)\n",
        "\n",
        "  for keyword, question in zip(keywords, questions):\n",
        "    distractors = distractor_generator.get_distractors(keyword, question)\n",
        "    print(f\"Distractors for {keyword}: {distractors}\")\n",
        "\n",
        "  class QuestionAnswerGeneratorWithGradio:\n",
        "      \"\"\"\n",
        "\n",
        "      Attributes:\n",
        "        text_summarizer:\n",
        "        keyword_extractor:\n",
        "        question_generator:\n",
        "        s2v_model:\n",
        "        sentence_model:\n",
        "        distractor_generator:\n",
        "      \"\"\"\n",
        "      def __init__(self):\n",
        "          \"\"\"\n",
        "          Initializes the QuestionAnswerGeneratorWithGradio with necessary models for text summarization,\n",
        "          keyword extraction, question generation, and distractor generation.\n",
        "          \"\"\"\n",
        "          # Initialize the text summarizer\n",
        "          self.text_summarizer = TextSummarizer()\n",
        "\n",
        "          # Initialize the keyword extractor\n",
        "          self.keyword_extractor = KeywordExtractor()\n",
        "\n",
        "          # Initialize the question generator\n",
        "          self.question_generator = QuestionGenerator()\n",
        "\n",
        "          # Load the Sense2Vec model from disk\n",
        "          self.s2v_model = Sense2Vec().from_disk('s2v_old')\n",
        "\n",
        "          # Initialize the sentence transformer model\n",
        "          self.sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
        "\n",
        "          # Initialize the distractor generator with the models\n",
        "          self.distractor_generator = DistractorGenerator(self.s2v_model, self.sentence_model)\n",
        "\n",
        "      def generate_questions_and_answers(self, content):\n",
        "          summary = self.text_summarizer.summarize_text(content)\n",
        "          keywords = self.keyword_extractor.get_keywords(content, summary)\n",
        "          questions = [self.question_generator.generate_question(summary, keyword) for keyword in keywords]\n",
        "          distractors = {keyword: self.distractor_generator.get_distractors(keyword, questions[i]) for i, keyword in enumerate(keywords)}\n",
        "          return summary, questions, distractors\n",
        "\n",
        "      def generate_output(self, content):\n",
        "          summary, questions, distractors = self.generate_questions_and_answers(content)\n",
        "          result = {\n",
        "              \"Summary\": summary,\n",
        "              \"Questions and Distractors\": [\n",
        "                  {\n",
        "                      \"Question\": questions[i],\n",
        "                      \"Distractors\": distractors[keyword]\n",
        "                  } for i, keyword in enumerate(keywords)\n",
        "              ]\n",
        "          }\n",
        "          return result\n",
        "\n",
        "\n",
        "  def qa_pipeline(input_text):\n",
        "      \"\"\"\n",
        "      Processes the input text to generate a summary, questions, and distractors, and formats the output.\n",
        "\n",
        "      Parameters:\n",
        "      input_text (str): The input text content to process.\n",
        "\n",
        "      Returns:\n",
        "      str: A string containing the summary, questions, and their distractors.\n",
        "      \"\"\"\n",
        "\n",
        "  s2v_model = Sense2Vec().from_disk('s2v_old')\n",
        "  sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
        "  distractor_generator = DistractorGenerator(s2v_model, sentence_model)\n",
        "\n",
        "  for keyword, question in zip(keywords, questions):\n",
        "      distractors = distractor_generator.get_distractors(keyword, question)\n",
        "      print(f\"Distractors for {keyword}: {distractors}\")\n",
        "\n",
        "\n",
        "  qa_generator = QuestionAnswerGeneratorWithGradio()\n",
        "  summary, questions, distractors = qa_generator.generate_questions_and_answers(sample_text)\n",
        "\n",
        "  generator = QuestionAnswerGeneratorWithGradio()\n",
        "  summary, questions, distractors = generator.generate_questions_and_answers(input_text)\n",
        "  output = f\"Summary:\\n{summary}\\n\\n\"\n",
        "  for i, question in enumerate(questions):\n",
        "    if i!=len(questions)-1:\n",
        "      output += f\"Q{i+1}: {question}\\n\"\n",
        "      output += f\"Distractors: {', '.join(distractors[keywords[i]])}\\n\\n\"\n",
        "  return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "3b6b312aa4c64ce8bbdd87dfb1d40665",
            "c9d7104df2fb4c219312375bb67f4966",
            "a2e42c5f4c914f6a883eb2154061b47c",
            "90d6ebd94f13496b9d49297bb1ec5bde",
            "b8937491d59847eba0cf313ffd8aaff1"
          ]
        },
        "id": "w3dGjLws-q7v",
        "outputId": "3b4d87b2-590c-46f2-cf9e-b6a7b27731ae"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b6b312aa4c64ce8bbdd87dfb1d40665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d7104df2fb4c219312375bb67f4966",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2e42c5f4c914f6a883eb2154061b47c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90d6ebd94f13496b9d49297bb1ec5bde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8937491d59847eba0cf313ffd8aaff1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "anvil.server.wait_forever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jp-Be1az96HR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}